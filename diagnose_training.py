"""
è¯Šæ–­è®­ç»ƒæ•°æ®å’Œç¥ç»ç½‘ç»œè´¨é‡
æ‰¾å‡ºä¸ºä»€ä¹ˆNNæ²¡æœ‰æå‡æ€§èƒ½
"""
import numpy as np
import matplotlib.pyplot as plt
import pickle
import torch
from neural_heuristic import NeuralHeuristic, load_trained_model
from reed_shepp import ReedsShepp

def diagnose_training_data():
    """æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡"""
    
    print("="*70)
    print("ğŸ” è®­ç»ƒæ•°æ®è´¨é‡è¯Šæ–­")
    print("="*70)
    
    # 1. åŠ è½½è®­ç»ƒæ•°æ®
    try:
        with open('data/training_data.pkl', 'rb') as f:
            data = pickle.load(f)
        print(f"\nâœ“ æˆåŠŸåŠ è½½è®­ç»ƒæ•°æ®")
    except FileNotFoundError:
        print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ° data/training_data.pkl")
        print(f"   è¯·å…ˆè¿è¡Œ: python data_generator.py")
        return
    
    X = np.array(data['features'])
    y = np.array(data['labels'])
    
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  æ ·æœ¬æ•°é‡: {len(X)}")
    print(f"  ç‰¹å¾ç»´åº¦: {X.shape[1]}")
    print(f"  æ ‡ç­¾ç»´åº¦: {y.shape}")
    
    # 2. æ£€æŸ¥ç‰¹å¾åˆ†å¸ƒ
    print(f"\nğŸ“ˆ ç‰¹å¾ç»Ÿè®¡:")
    feature_names = ['Î”x', 'Î”y', 'sin(Î”Î¸)', 'cos(Î”Î¸)', 'RS_dist', 
                     'E_l', 'E_w', 'E_wb', 'Ï†_max', 'SCS', 'is_parallel']
    
    for i, name in enumerate(feature_names):
        values = X[:, i]
        print(f"  {name:12s}: mean={np.mean(values):8.4f}, "
              f"std={np.std(values):8.4f}, "
              f"range=[{np.min(values):8.4f}, {np.max(values):8.4f}]")
    
    # 3. æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    print(f"\nğŸ¯ æ ‡ç­¾ç»Ÿè®¡ (çœŸå®cost-to-go):")
    print(f"  Mean: {np.mean(y):.4f}")
    print(f"  Std:  {np.std(y):.4f}")
    print(f"  Min:  {np.min(y):.4f}")
    print(f"  Max:  {np.max(y):.4f}")
    print(f"  Median: {np.median(y):.4f}")
    
    # 4. æ£€æŸ¥RSè·ç¦» vs çœŸå®costçš„å…³ç³»
    rs_distances = X[:, 4]  # RS distanceæ˜¯ç¬¬5ä¸ªç‰¹å¾
    
    print(f"\nğŸ”— RSè·ç¦» vs çœŸå®Costå…³ç³»:")
    ratio = y / (rs_distances + 1e-6)
    print(f"  y/RS_dist ratio mean: {np.mean(ratio):.4f}")
    print(f"  y/RS_dist ratio std:  {np.std(ratio):.4f}")
    
    if np.mean(ratio) > 1.5:
        print(f"  âš ï¸  è­¦å‘Š: çœŸå®costè¿œå¤§äºRSè·ç¦» (ratio > 1.5)")
        print(f"      è¿™å¯èƒ½è¡¨ç¤ºè®­ç»ƒæ•°æ®æ¥è‡ªå¾ˆå¤æ‚çš„åœºæ™¯")
    elif np.mean(ratio) < 0.8:
        print(f"  âš ï¸  è­¦å‘Š: çœŸå®costå°äºRSè·ç¦» (ratio < 0.8)")
        print(f"      è¿™ä¸å¤ªå¯èƒ½,RSåº”è¯¥æ˜¯ä¸‹ç•Œ")
    else:
        print(f"  âœ“ æ¯”ç‡åˆç† (0.8 < ratio < 1.5)")
    
    # 5. æ£€æŸ¥åœºæ™¯åˆ†å¸ƒ
    is_parallel = X[:, 10]
    n_parallel = np.sum(is_parallel == 1)
    n_perpendicular = np.sum(is_parallel == 0)
    
    print(f"\nğŸ…¿ï¸  åœºæ™¯ç±»å‹åˆ†å¸ƒ:")
    print(f"  Parallel parking:       {n_parallel:4d} ({n_parallel/len(X)*100:.1f}%)")
    print(f"  Perpendicular parking:  {n_perpendicular:4d} ({n_perpendicular/len(X)*100:.1f}%)")
    
    if abs(n_parallel - n_perpendicular) > len(X) * 0.3:
        print(f"  âš ï¸  è­¦å‘Š: åœºæ™¯åˆ†å¸ƒä¸å¹³è¡¡")
        print(f"      å»ºè®®: å¢åŠ å°‘æ•°ç±»åˆ«çš„æ ·æœ¬")
    
    # 6. æ£€æŸ¥SCSåˆ†å¸ƒ
    scs_values = X[:, 9]
    print(f"\nğŸ“ SCS (ç©ºé—´çº¦æŸ) åˆ†å¸ƒ:")
    print(f"  Mean SCS: {np.mean(scs_values):.4f}")
    print(f"  Range: [{np.min(scs_values):.4f}, {np.max(scs_values):.4f}]")
    
    unique_scs = np.unique(scs_values)
    print(f"  Unique SCS values: {unique_scs}")
    
    if len(unique_scs) < 5:
        print(f"  âš ï¸  è­¦å‘Š: SCSå€¼ç§ç±»å¤ªå°‘ ({len(unique_scs)})")
        print(f"      å»ºè®®: ç”Ÿæˆæ›´å¤šä¸åŒSCSçš„åœºæ™¯")
    
    # 7. å¯è§†åŒ–
    visualize_training_data(X, y, rs_distances)
    
    return X, y

def diagnose_neural_network():
    """æ£€æŸ¥ç¥ç»ç½‘ç»œé¢„æµ‹è´¨é‡"""
    
    print("\n" + "="*70)
    print("ğŸ§  ç¥ç»ç½‘ç»œé¢„æµ‹è´¨é‡è¯Šæ–­")
    print("="*70)
    
    # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨æ–°çš„wrapperï¼‰
    try:
        from neural_heuristic import create_neural_heuristic
        neural_heuristic = create_neural_heuristic('models/neural_heuristic.pth')
        print(f"\nâœ“ æˆåŠŸåŠ è½½ç¥ç»ç½‘ç»œæ¨¡å‹")
    except FileNotFoundError:
        print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹")
        print(f"   è¯·å…ˆè¿è¡Œ: python train_nn.py")
        return
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        return
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    with open('data/training_data.pkl', 'rb') as f:
        data = pickle.load(f)
    
    X = np.array(data['features'])
    y = np.array(data['labels'])
    
    # ä½¿ç”¨æœ€å20%ä½œä¸ºæµ‹è¯•
    split_idx = int(len(X) * 0.8)
    X_test = X[split_idx:]
    y_test = y[split_idx:]
    
    # é¢„æµ‹ï¼ˆä½¿ç”¨wrapperçš„predictæ–¹æ³•ï¼‰
    predictions = []
    for i in range(len(X_test)):
        features = X_test[i]
        # ç‰¹å¾é¡ºåº: [dx, dy, sin(dtheta), cos(dtheta), rs_dist, E_l, E_w, E_wb, phi_max, scs, is_parallel]
        current_state = np.array([0.0, 0.0, 0.0])  # dummy
        goal_state = np.array([features[0], features[1], np.arctan2(features[2], features[3])])
        rs_dist = features[4]
        vehicle_params = [features[5], features[6], features[7], features[8]]
        scs = features[9]
        is_parallel = bool(features[10] > 0.5)
        
        pred = neural_heuristic.predict(current_state, goal_state, rs_dist, vehicle_params, scs, is_parallel)
        predictions.append(pred)
    
    predictions = np.array(predictions)
    # åˆ†æé¢„æµ‹è´¨é‡
    print(f"\nğŸ“Š æµ‹è¯•é›†é¢„æµ‹ç»Ÿè®¡:")
    print(f"  æ ·æœ¬æ•°: {len(y_test)}")
    
    errors = predictions - y_test
    abs_errors = np.abs(errors)
    rel_errors = abs_errors / (y_test + 1e-6) * 100
    
    print(f"\nğŸ“ é¢„æµ‹è¯¯å·®:")
    print(f"  Mean Absolute Error: {np.mean(abs_errors):.4f}")
    print(f"  Std of errors:       {np.std(errors):.4f}")
    print(f"  Max error:           {np.max(abs_errors):.4f}")
    print(f"  Median error:        {np.median(abs_errors):.4f}")
    
    print(f"\nğŸ“Š ç›¸å¯¹è¯¯å·® (%):")
    print(f"  Mean: {np.mean(rel_errors):.2f}%")
    print(f"  Std:  {np.std(rel_errors):.2f}%")
    print(f"  Max:  {np.max(rel_errors):.2f}%")
    
    # æ£€æŸ¥æ˜¯å¦ç³»ç»Ÿæ€§é«˜ä¼°æˆ–ä½ä¼°
    print(f"\nğŸ¯ é¢„æµ‹åå·®:")
    mean_error = np.mean(errors)
    if mean_error > 0.5:
        print(f"  âš ï¸  ç³»ç»Ÿæ€§é«˜ä¼°: {mean_error:.4f}")
        print(f"      NNé¢„æµ‹çš„costé«˜äºçœŸå®å€¼")
        print(f"      â†’ ä¼šå¯¼è‡´è¿‡åº¦è°¨æ…,ç”Ÿæˆæ›´å¤šèŠ‚ç‚¹")
    elif mean_error < -0.5:
        print(f"  âš ï¸  ç³»ç»Ÿæ€§ä½ä¼°: {mean_error:.4f}")
        print(f"      NNé¢„æµ‹çš„costä½äºçœŸå®å€¼")
        print(f"      â†’ è¿åadmissibility,å¯èƒ½æ‰¾ä¸åˆ°æœ€ä¼˜è§£")
    else:
        print(f"  âœ“ é¢„æµ‹åŸºæœ¬æ— å: {mean_error:.4f}")
    
    # æŒ‰åœºæ™¯ç±»å‹åˆ†æ
    print(f"\nğŸ” æŒ‰åœºæ™¯ç±»å‹åˆ†æ:")
    
    is_parallel_test = X_test[:, 10]
    
    # Parallel parking
    parallel_mask = is_parallel_test == 1
    if np.sum(parallel_mask) > 0:
        parallel_errors = abs_errors[parallel_mask]
        print(f"  Parallel parking:")
        print(f"    MAE: {np.mean(parallel_errors):.4f}")
        print(f"    ç›¸å¯¹è¯¯å·®: {np.mean(rel_errors[parallel_mask]):.2f}%")
    
    # Perpendicular parking
    perp_mask = is_parallel_test == 0
    if np.sum(perp_mask) > 0:
        perp_errors = abs_errors[perp_mask]
        print(f"  Perpendicular parking:")
        print(f"    MAE: {np.mean(perp_errors):.4f}")
        print(f"    ç›¸å¯¹è¯¯å·®: {np.mean(rel_errors[perp_mask]):.2f}%")
        
        # å…³é”®å‘ç°
        if np.mean(perp_errors) > np.mean(parallel_errors) * 1.5:
            print(f"  âš ï¸  å‚ç›´æ³Šè½¦è¯¯å·®æ˜æ˜¾æ›´å¤§!")
            print(f"      è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆperpendicularåœºæ™¯æ€§èƒ½æ›´å·®")
    
    # å¯è§†åŒ–é¢„æµ‹è´¨é‡
    visualize_predictions(y_test, predictions, X_test)
    
    return predictions, y_test

def visualize_training_data(X, y, rs_distances):
    """å¯è§†åŒ–è®­ç»ƒæ•°æ®"""
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. æ ‡ç­¾åˆ†å¸ƒ
    ax = axes[0, 0]
    ax.hist(y, bins=50, alpha=0.7, color='blue', edgecolor='black')
    ax.set_xlabel('True Cost-to-go', fontsize=12)
    ax.set_ylabel('Frequency', fontsize=12)
    ax.set_title('Distribution of Training Labels', fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # 2. RSè·ç¦» vs çœŸå®Cost
    ax = axes[0, 1]
    ax.scatter(rs_distances, y, alpha=0.5, s=20)
    ax.plot([0, np.max(rs_distances)], [0, np.max(rs_distances)], 
            'r--', label='y = RS_dist', linewidth=2)
    ax.set_xlabel('Reed-Shepp Distance', fontsize=12)
    ax.set_ylabel('True Cost-to-go', fontsize=12)
    ax.set_title('RS Distance vs True Cost', fontsize=13, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 3. åœºæ™¯ç±»å‹å¯¹æ¯”
    ax = axes[1, 0]
    is_parallel = X[:, 10]
    parallel_costs = y[is_parallel == 1]
    perp_costs = y[is_parallel == 0]
    
    data_to_plot = [parallel_costs, perp_costs]
    ax.boxplot(data_to_plot, labels=['Parallel', 'Perpendicular'])
    ax.set_ylabel('Cost-to-go', fontsize=12)
    ax.set_title('Cost Distribution by Scenario Type', fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    # 4. SCS vs Cost
    ax = axes[1, 1]
    scs_values = X[:, 9]
    ax.scatter(scs_values, y, alpha=0.5, s=20, c=is_parallel, cmap='viridis')
    ax.set_xlabel('SCS (Space Constraint Scale)', fontsize=12)
    ax.set_ylabel('True Cost-to-go', fontsize=12)
    ax.set_title('SCS vs Cost (color=scenario type)', fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/training_data_diagnosis.png', dpi=150, bbox_inches='tight')
    print(f"\nâœ“ è®­ç»ƒæ•°æ®å¯è§†åŒ–ä¿å­˜åˆ°: results/training_data_diagnosis.png")
    plt.close()

def visualize_predictions(y_true, y_pred, X_test):
    """å¯è§†åŒ–é¢„æµ‹è´¨é‡"""
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. é¢„æµ‹ vs çœŸå®å€¼
    ax = axes[0, 0]
    ax.scatter(y_true, y_pred, alpha=0.5, s=30)
    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 
            'r--', linewidth=2, label='Perfect Prediction')
    ax.set_xlabel('True Cost', fontsize=12)
    ax.set_ylabel('Predicted Cost', fontsize=12)
    ax.set_title('Prediction vs Ground Truth', fontsize=13, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. è¯¯å·®åˆ†å¸ƒ
    ax = axes[0, 1]
    errors = y_pred - y_true
    ax.hist(errors, bins=50, alpha=0.7, color='orange', edgecolor='black')
    ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')
    ax.axvline(x=np.mean(errors), color='blue', linestyle='--', linewidth=2, 
              label=f'Mean Error: {np.mean(errors):.3f}')
    ax.set_xlabel('Prediction Error (pred - true)', fontsize=12)
    ax.set_ylabel('Frequency', fontsize=12)
    ax.set_title('Error Distribution', fontsize=13, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 3. æŒ‰åœºæ™¯ç±»å‹çš„è¯¯å·®
    ax = axes[1, 0]
    is_parallel = X_test[:, 10]
    parallel_errors = np.abs(errors[is_parallel == 1])
    perp_errors = np.abs(errors[is_parallel == 0])
    
    data = [parallel_errors, perp_errors]
    ax.boxplot(data, labels=['Parallel', 'Perpendicular'])
    ax.set_ylabel('Absolute Error', fontsize=12)
    ax.set_title('Prediction Error by Scenario Type', fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    # 4. ç›¸å¯¹è¯¯å·®
    ax = axes[1, 1]
    rel_errors = np.abs(errors) / (y_true + 1e-6) * 100
    ax.hist(rel_errors, bins=50, alpha=0.7, color='green', edgecolor='black')
    ax.set_xlabel('Relative Error (%)', fontsize=12)
    ax.set_ylabel('Frequency', fontsize=12)
    ax.set_title(f'Relative Error Distribution (Mean: {np.mean(rel_errors):.1f}%)', 
                fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/prediction_quality_diagnosis.png', dpi=150, bbox_inches='tight')
    print(f"âœ“ é¢„æµ‹è´¨é‡å¯è§†åŒ–ä¿å­˜åˆ°: results/prediction_quality_diagnosis.png")
    plt.close()

def check_inference_speed():
    """æ£€æŸ¥æ¨ç†é€Ÿåº¦"""
    
    print("\n" + "="*70)
    print("âš¡ æ¨ç†é€Ÿåº¦æµ‹è¯•")
    print("="*70)
    
    import time
    from reed_shepp import ReedsShepp
    
    # åŠ è½½æ¨¡å‹
    model = load_trained_model('models/neural_heuristic.pth')
    model.eval()
    
    rs = ReedsShepp()
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = np.random.rand(1, 11).astype(np.float32)
    test_tensor = torch.FloatTensor(test_input)
    
    # æµ‹è¯•NNæ¨ç†é€Ÿåº¦
    n_iterations = 1000
    
    start_time = time.time()
    with torch.no_grad():
        for _ in range(n_iterations):
            _ = model(test_tensor)
    nn_time = (time.time() - start_time) / n_iterations * 1000
    
    # æµ‹è¯•RSè·ç¦»è®¡ç®—é€Ÿåº¦
    start = np.array([0.0, 0.0, 0.0])
    goal = np.array([5.0, 5.0, 1.57])
    
    start_time = time.time()
    for _ in range(n_iterations):
        _ = rs.distance(start, goal)
    rs_time = (time.time() - start_time) / n_iterations * 1000
    
    print(f"\nâ±ï¸  æ¯æ¬¡è°ƒç”¨æ—¶é—´:")
    print(f"  ç¥ç»ç½‘ç»œæ¨ç†: {nn_time:.4f} ms")
    print(f"  RSè·ç¦»è®¡ç®—:   {rs_time:.4f} ms")
    print(f"  é€Ÿåº¦æ¯”: NN is {nn_time/rs_time:.1f}x slower than RS")
    
    # ä¼°ç®—æ•´ä½“å½±å“
    avg_nodes_per_search = 1500
    total_overhead = nn_time * avg_nodes_per_search
    
    print(f"\nğŸ“Š å¯¹æ•´ä½“æœç´¢çš„å½±å“:")
    print(f"  å‡è®¾å¹³å‡æœç´¢{avg_nodes_per_search}ä¸ªèŠ‚ç‚¹")
    print(f"  NNæ€»å¼€é”€: {total_overhead:.1f} ms")
    
    if total_overhead > 100:
        print(f"  âš ï¸  è­¦å‘Š: NNå¼€é”€ ({total_overhead:.0f}ms) å¾ˆå¤§!")
        print(f"      è¿™å¯èƒ½å®Œå…¨æŠµæ¶ˆæœç´¢æ•ˆç‡æå‡")

def main():
    """è¿è¡Œæ‰€æœ‰è¯Šæ–­"""
    
    print("\n" + "ğŸ”¬"*35)
    print("ç¥ç»ç½‘ç»œæ³Šè½¦è§„åˆ’ - å®Œæ•´è¯Šæ–­")
    print("ğŸ”¬"*35 + "\n")
    
    # 1. è®­ç»ƒæ•°æ®è¯Šæ–­
    X, y = diagnose_training_data()
    
    # 2. ç¥ç»ç½‘ç»œè¯Šæ–­
    predictions, y_test = diagnose_neural_network()
    
    # 3. æ¨ç†é€Ÿåº¦æµ‹è¯•
    check_inference_speed()
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“ è¯Šæ–­æ€»ç»“")
    print("="*70)
    print("\næŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶è·å–è¯¦ç»†åˆ†æ:")
    print("  1. results/training_data_diagnosis.png - è®­ç»ƒæ•°æ®è´¨é‡")
    print("  2. results/prediction_quality_diagnosis.png - é¢„æµ‹è´¨é‡")
    print("\nå¸¸è§é—®é¢˜:")
    print("  âœ“ è®­ç»ƒæ•°æ®ä¸å¤Ÿå¤š â†’ å¢åŠ æ ·æœ¬")
    print("  âœ“ åœºæ™¯åˆ†å¸ƒä¸å¹³è¡¡ â†’ å¹³è¡¡parallel/perpendicular")
    print("  âœ“ SCSå€¼ç§ç±»å¤ªå°‘ â†’ ç”Ÿæˆæ›´å¤šä¸åŒSCS")
    print("  âœ“ é¢„æµ‹è¯¯å·®å¤ªå¤§ â†’ æ”¹è¿›æ¨¡å‹æ¶æ„/è®­ç»ƒ")
    print("  âœ“ æ¨ç†å¤ªæ…¢ â†’ æ¨¡å‹ä¼˜åŒ–/é‡åŒ–")

if __name__ == '__main__':
    main()